{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJs-tG2qn5A1"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) ## Check the shape\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)) ## Check the shape\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "print(len(train_images))\n",
        "print(len(train_labels))"
      ],
      "metadata": {
        "id": "5Je5wD0LoF6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_1 = []\n",
        "train_labels_1 = []\n",
        "train_images_2 = []\n",
        "train_labels_2 = []\n",
        "\n",
        "\n",
        "digit_counts = {digit: 0 for digit in range(5, 10)}\n",
        "\n",
        "for i, digit in enumerate(train_labels):\n",
        "    if digit <= 4:\n",
        "        train_images_1.append(train_images[i])\n",
        "        train_labels_1.append(digit)\n",
        "    elif 5 <= digit <= 9 and digit_counts[digit] < 10:\n",
        "        train_images_2.append(train_images[i])\n",
        "        train_labels_2.append(digit)\n",
        "        digit_counts[digit] += 1\n",
        "\n",
        "\n",
        "print(len(train_images_1))\n",
        "print(len(train_labels_1)) #Ensuring same length, and totaling about 30k.\n",
        "\n",
        "print(len(train_images_2))\n",
        "print(len(train_labels_2)) #Ensuring 50 count (10 of each digit > 4)"
      ],
      "metadata": {
        "id": "ks97pGzBoU_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_1 = []\n",
        "test_labels_1 = []\n",
        "test_images_2 = []\n",
        "test_labels_2 = []\n",
        "\n",
        "for i, digit in enumerate(test_labels):\n",
        "    if digit <= 4:\n",
        "      test_images_1.append(test_images[i])\n",
        "      test_labels_1.append(digit)\n",
        "    else:\n",
        "      test_images_2.append(test_images[i])\n",
        "      test_labels_2.append(digit)\n",
        "\n",
        "print(len(test_images_1))\n",
        "print(len(test_labels_1))\n",
        "\n",
        "print(len(test_images_2))\n",
        "print(len(test_labels_2)) #Ensuring same length, total 5k each."
      ],
      "metadata": {
        "id": "Hlj0GWeV9_Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffling training set 1 and 2:\n",
        "import numpy as np\n",
        "\n",
        "perm_1 = np.random.permutation(len(train_images_1))\n",
        "train_images_1 = np.array(train_images_1)\n",
        "train_labels_1 = np.array(train_labels_1) #Need to be numpy arrays\n",
        "\n",
        "perm_2 = np.random.permutation(len(train_images_2))\n",
        "train_images_2 = np.array(train_images_2)\n",
        "train_labels_2 = np.array(train_labels_2) #Need to be numpy arrays\n",
        "\n",
        "train_images_1 = train_images_1[perm_1]\n",
        "train_labels_1 = train_labels_1[perm_1]\n",
        "\n",
        "train_images_2 = train_images_2[perm_2]\n",
        "train_labels_2 = train_labels_2[perm_2]\n",
        "\n",
        "\n",
        "\n",
        "#Making test_data a numpy array so they can have shape\n",
        "test_images_1 = np.array(test_images_1)\n",
        "test_images_2 = np.array(test_images_2)\n",
        "test_labels_1 = np.array(test_labels_1)\n",
        "test_labels_2 = np.array(test_labels_2)\n",
        "\n",
        "print(len(train_images_1))\n",
        "print(len(train_images_2))"
      ],
      "metadata": {
        "id": "8jZdU1Nw_Aro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating validation data (20% of train)\n",
        "val_images_1 = train_images_1[:int(.2*len(train_images_1))] #Takes first 20%\n",
        "train_images_1 = train_images_1[int(.2*len(train_images_1)):] #Assigns training images last 80%\n",
        "\n",
        "val_labels_1 = train_labels_1[:int(.2*len(train_labels_1))] #Takes first 20%\n",
        "train_labels_1 = train_labels_1[int(.2*len(train_labels_1)):] #Assigns training labels last 80%\n",
        "\n",
        "val_images_2 = train_images_2[:int(.2*len(train_images_2))] #Repeat for 2\n",
        "train_images_2 = train_images_2[int(.2*len(train_images_2)):]\n",
        "\n",
        "val_labels_2 = train_labels_2[:int(.2*len(train_labels_2))]\n",
        "train_labels_2 = train_labels_2[int(.2*len(train_labels_2)):]"
      ],
      "metadata": {
        "id": "PwhKa4CDCBlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 1\n",
        "inputs = keras.Input(shape=(28, 28, 1)) ## Different from densenet input\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_1 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_1.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "#Creating ModelCheckpoint callback\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\n",
        "filepath=\"Model_1.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model_1.fit(train_images_1, train_labels_1,\n",
        "epochs=30,\n",
        "validation_data=(val_images_1,val_labels_1),\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "IPbygSxF_BNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print training/validation results for Model 1 at optimal epochs determined by ModelCheckpoint\n",
        "test_model_1 = keras.models.load_model(\"Model_1.keras\")\n",
        "train_loss, train_acc = test_model_1.evaluate(train_images_1,train_labels_1)\n",
        "test_loss, test_acc = test_model_1.evaluate(test_images_1,test_labels_1)\n",
        "print(\"Model 1:\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "VIkubg5eENz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 2\n",
        "inputs = keras.Input(shape=(28, 28, 1)) ## Different from densenet input\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_2 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_2.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "#Creating ModelCheckpoint callback\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\n",
        "filepath=\"Model_2.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model_2.fit(train_images_2, train_labels_2,\n",
        "epochs=30,\n",
        "validation_data=(val_images_2,val_labels_2),\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "oJXqu3SdNjHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print training/validation results for Model 2 at optimal epochs determined by ModelCheckpoint\n",
        "test_model_2 = keras.models.load_model(\"Model_2.keras\")\n",
        "train_loss, train_acc = test_model_2.evaluate(train_images_2,train_labels_2)\n",
        "test_loss, test_acc = test_model_2.evaluate(test_images_2,test_labels_2)\n",
        "print(\"Model 2:\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "O7kg-58LNma-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Augmentation Approach\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for i in range(len(train_images_2)):\n",
        "    for _ in range(10): #Creates 10 new pictures per original\n",
        "        augmented_image = data_augmentation(\n",
        "            np.expand_dims(train_images_2[i], axis=0)\n",
        "        )\n",
        "        augmented_images.append(augmented_image[0])\n",
        "        augmented_labels.append(train_labels_2[i])\n",
        "\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "train_images_2_new = np.concatenate((train_images_2, augmented_images))\n",
        "train_labels_2_new = np.concatenate((train_labels_2, augmented_labels))\n",
        "\n",
        "#Shuffling\n",
        "perm = np.random.permutation(len(train_images_2_new))\n",
        "train_images_2_new = train_images_2_new[perm]\n",
        "train_labels_2_new = train_labels_2_new[perm]\n",
        "\n",
        "\n",
        "#Creating new val and train\n",
        "val_images_2_new = train_images_2_new[:int(.2*len(train_images_2_new))]\n",
        "train_images_2_new = train_images_2_new[int(.2*len(train_images_2_new)):]\n",
        "\n",
        "val_labels_2_new = train_labels_2_new[:int(.2*len(train_labels_2_new))]\n",
        "train_labels_2_new = train_labels_2_new[int(.2*len(train_labels_2_new)):]"
      ],
      "metadata": {
        "id": "9kCG_Z1PWCDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 3 (with augmentation)\n",
        "inputs = keras.Input(shape=(28, 28, 1)) ## Different from densenet input\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_3 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_3.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "#Creating ModelCheckpoint callback\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\n",
        "filepath=\"Model_3.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model_3.fit(train_images_2_new, train_labels_2_new,\n",
        "epochs=30,\n",
        "validation_data=(val_images_2_new,val_labels_2_new),\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "ku4FiYjcYjgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print training/validation results for Model 3 at optimal epochs determined by ModelCheckpoint\n",
        "test_model_3 = keras.models.load_model(\"Model_3.keras\")\n",
        "train_loss, train_acc = test_model_3.evaluate(train_images_2_new,train_labels_2_new)\n",
        "test_loss, test_acc = test_model_3.evaluate(test_images_2,test_labels_2)\n",
        "print(\"Model 3:\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "Uc1pcpjTY4bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images_2_new)) #(40 original + 400 augmented) * .8 = 352"
      ],
      "metadata": {
        "id": "KP7yrczaZRug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New data augmentation: Model 4\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),  #Flips both horizontally and vertically\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),  #Shift up to 10% of the height/width\n",
        "        layers.RandomRotation(0.2),  #Testing Rotation\n",
        "        layers.RandomContrast(0.2),  #Testing Contrast\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for i in range(len(train_images_2)):\n",
        "    for _ in range(20): #Trying 20 pictures per\n",
        "        augmented_image = data_augmentation(\n",
        "            np.expand_dims(train_images_2[i], axis=0)\n",
        "        )\n",
        "        augmented_images.append(augmented_image[0])\n",
        "        augmented_labels.append(train_labels_2[i])\n",
        "\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "train_images_2_new = np.concatenate((train_images_2, augmented_images))\n",
        "train_labels_2_new = np.concatenate((train_labels_2, augmented_labels))\n",
        "\n",
        "#Shuffling\n",
        "perm = np.random.permutation(len(train_images_2_new))\n",
        "train_images_2_new = train_images_2_new[perm]\n",
        "train_labels_2_new = train_labels_2_new[perm]\n",
        "\n",
        "\n",
        "#Creating new val and train\n",
        "val_images_2_new = train_images_2_new[:int(.2*len(train_images_2_new))]\n",
        "train_images_2_new = train_images_2_new[int(.2*len(train_images_2_new)):]\n",
        "\n",
        "val_labels_2_new = train_labels_2_new[:int(.2*len(train_labels_2_new))]\n",
        "train_labels_2_new = train_labels_2_new[int(.2*len(train_labels_2_new)):]"
      ],
      "metadata": {
        "id": "-irObQdraG1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 4 (with augmentation)\n",
        "inputs = keras.Input(shape=(28, 28, 1)) ## Different from densenet input\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_4 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_4.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "#Creating ModelCheckpoint callback\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\n",
        "filepath=\"Model_4.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model_4.fit(train_images_2_new, train_labels_2_new,\n",
        "epochs=30,\n",
        "validation_data=(val_images_2_new,val_labels_2_new),\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "CplekrGlbrKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print training/validation results for Model 4 at optimal epochs determined by ModelCheckpoint\n",
        "test_model_4 = keras.models.load_model(\"Model_4.keras\")\n",
        "train_loss, train_acc = test_model_4.evaluate(train_images_2_new,train_labels_2_new)\n",
        "test_loss, test_acc = test_model_4.evaluate(test_images_2,test_labels_2)\n",
        "print(\"Model 4:\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "w1P6BpTJbyc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images_2_new)) #(40 original + 800 augmented) * .8 = 672"
      ],
      "metadata": {
        "id": "5Kl8lH9Ab1pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New data augmentation: Model 5\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.15),\n",
        "        layers.RandomZoom(height_factor=(0.1, 0.3)),\n",
        "        layers.RandomBrightness(factor=0.2),  #Testing brightness\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for i in range(len(train_images_2)):\n",
        "    for _ in range(30): #Trying 20 pictures per\n",
        "        augmented_image = data_augmentation(\n",
        "            np.expand_dims(train_images_2[i], axis=0)\n",
        "        )\n",
        "        augmented_images.append(augmented_image[0])\n",
        "        augmented_labels.append(train_labels_2[i])\n",
        "\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "train_images_2_new = np.concatenate((train_images_2, augmented_images))\n",
        "train_labels_2_new = np.concatenate((train_labels_2, augmented_labels))\n",
        "\n",
        "#Shuffling\n",
        "perm = np.random.permutation(len(train_images_2_new))\n",
        "train_images_2_new = train_images_2_new[perm]\n",
        "train_labels_2_new = train_labels_2_new[perm]\n",
        "\n",
        "\n",
        "#Creating new val and train\n",
        "val_images_2_new = train_images_2_new[:int(.2*len(train_images_2_new))]\n",
        "train_images_2_new = train_images_2_new[int(.2*len(train_images_2_new)):]\n",
        "\n",
        "val_labels_2_new = train_labels_2_new[:int(.2*len(train_labels_2_new))]\n",
        "train_labels_2_new = train_labels_2_new[int(.2*len(train_labels_2_new)):]"
      ],
      "metadata": {
        "id": "cl-VFhG3cFN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 5 (with augmentation)\n",
        "inputs = keras.Input(shape=(28, 28, 1)) ## Different from densenet input\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_5 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_5.compile(loss=\"SparseCategoricalCrossentropy\", optimizer=\"rmsprop\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "#Creating ModelCheckpoint callback\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\n",
        "filepath=\"Model_5.keras\",\n",
        "save_best_only=True,\n",
        "monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model_5.fit(train_images_2_new, train_labels_2_new,\n",
        "epochs=30,\n",
        "validation_data=(val_images_2_new,val_labels_2_new),\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "id": "yaLci9Y7cXhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print training/validation results for Model 4 at optimal epochs determined by ModelCheckpoint\n",
        "test_model_5 = keras.models.load_model(\"Model_5.keras\")\n",
        "train_loss, train_acc = test_model_5.evaluate(train_images_2_new,train_labels_2_new)\n",
        "test_loss, test_acc = test_model_5.evaluate(test_images_2,test_labels_2)\n",
        "print(\"Model 5:\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "pRKGxI5wcdcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images_2_new)) #(40 original + 1200 augmented) * .8 = 992"
      ],
      "metadata": {
        "id": "_u0EwKSYcqvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}